{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read the dataset\n",
    "def read_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a CSV file.\n",
    "    :param file_path: Path to the CSV file.\n",
    "    :return: Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(file_path, dtype=str, index_col = 0)\n",
    "    print(f\"Dataset loaded. Shape: {dataset.shape}\")\n",
    "    print(dataset.head(10))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Shape: (179950, 3)\n",
      "                 Number     Gender       Lemma\n",
      "Word                                          \n",
      "100-mètres   invariable  masculine  100-mètres\n",
      "2D           invariable   feminine          2D\n",
      "3D           invariable   feminine          3D\n",
      "µA           invariable  masculine           A\n",
      "a            invariable  masculine           a\n",
      "a b c          singular  masculine       a b c\n",
      " a demi-mot    singular  masculine  a demi-mot\n",
      "a-mi-la        singular  masculine     a-mi-la\n",
      "aa           invariable  masculine          aa\n",
      "aabam          singular  masculine       aabam\n"
     ]
    }
   ],
   "source": [
    "# File path to the nouns dataset\n",
    "file_path = r\"C:\\Users\\user1\\Desktop\\HarvestWE-main\\HarvestWE-main\\Data\\Morphalou\\all_nouns_v2.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "nouns = read_dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100-mètres</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>100-mètres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2D</th>\n",
       "      <td>invariable</td>\n",
       "      <td>feminine</td>\n",
       "      <td>2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3D</th>\n",
       "      <td>invariable</td>\n",
       "      <td>feminine</td>\n",
       "      <td>3D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>µA</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>φ</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>φ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>χ</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>χ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ψ</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>ψ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ω</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>ω</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ω</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Ω</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175708 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Number     Gender       Lemma\n",
       "Word                                         \n",
       "100-mètres  invariable  masculine  100-mètres\n",
       "2D          invariable   feminine          2D\n",
       "3D          invariable   feminine          3D\n",
       "µA          invariable  masculine           A\n",
       "a           invariable  masculine           a\n",
       "...                ...        ...         ...\n",
       "φ           invariable  masculine           φ\n",
       "χ           invariable  masculine           χ\n",
       "ψ           invariable  masculine           ψ\n",
       "ω           invariable  masculine           ω\n",
       "Ω           invariable  masculine           Ω\n",
       "\n",
       "[175708 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = nouns[nouns.Gender != 'invariable']\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the dataset\n",
    "def clean_dataset(dataset, feature_columns):\n",
    "    \"\"\"\n",
    "    Clean the dataset by removing rows with missing values in specified columns.\n",
    "    :param dataset: Input DataFrame.\n",
    "    :param feature_columns: List of columns to check for missing values.\n",
    "    :return: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    cleaned_dataset = dataset.dropna(subset=feature_columns)\n",
    "\n",
    "    print(f\"Dataset cleaned. Remaining rows: {cleaned_dataset.shape[0]}\")\n",
    "    return cleaned_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cleaned. Remaining rows: 175708\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100-mètres</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>100-mètres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2D</th>\n",
       "      <td>invariable</td>\n",
       "      <td>feminine</td>\n",
       "      <td>2D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3D</th>\n",
       "      <td>invariable</td>\n",
       "      <td>feminine</td>\n",
       "      <td>3D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>µA</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>φ</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>φ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>χ</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>χ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ψ</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>ψ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ω</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>ω</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ω</th>\n",
       "      <td>invariable</td>\n",
       "      <td>masculine</td>\n",
       "      <td>Ω</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175708 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Number     Gender       Lemma\n",
       "Word                                         \n",
       "100-mètres  invariable  masculine  100-mètres\n",
       "2D          invariable   feminine          2D\n",
       "3D          invariable   feminine          3D\n",
       "µA          invariable  masculine           A\n",
       "a           invariable  masculine           a\n",
       "...                ...        ...         ...\n",
       "φ           invariable  masculine           φ\n",
       "χ           invariable  masculine           χ\n",
       "ψ           invariable  masculine           ψ\n",
       "ω           invariable  masculine           ω\n",
       "Ω           invariable  masculine           Ω\n",
       "\n",
       "[175708 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Clean the dataset (focus on Gender column)\n",
    "nouns_cleaned  = clean_dataset(nouns, feature_columns=['Gender'])\n",
    "nouns_cleaned "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to encode features\n",
    "def encode_feature(feature):\n",
    "    \"\"\"\n",
    "    Encode categorical features as numeric values.\n",
    "    :param feature: Pandas Series to encode.\n",
    "    :return: Encoded feature.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    le.fit(feature.unique())\n",
    "    feature_encoded = le.transform(feature)\n",
    "    return feature_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, feature_name, encode_as1=None, normalize_columns=None, remove_original=False):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset by encoding features and normalizing specified columns.\n",
    "    :param dataset: Input DataFrame.\n",
    "    :param feature_name: The feature to encode as binary.\n",
    "    :param encode_as1: Map one of the feature's values to 1, others to 0.\n",
    "    :param normalize_columns: List of columns to normalize.\n",
    "    :param remove_original: Whether to remove the original categorical column.\n",
    "    :return: Preprocessed DataFrame.\n",
    "    \"\"\"\n",
    "    # Encode Gender\n",
    "    if encode_as1:\n",
    "        dataset[f\"{feature_name}_encoded\"] = (dataset[feature_name] == encode_as1).astype(int)\n",
    "    else:\n",
    "        dataset[f\"{feature_name}_encoded\"] = encode_feature(dataset[feature_name])\n",
    "\n",
    "    print(f\"Feature '{feature_name}' encoded. Sample:\")\n",
    "    print(dataset[[feature_name, f\"{feature_name}_encoded\"]].head(10))\n",
    "    \n",
    "    # Normalize specified columns\n",
    "    if normalize_columns:\n",
    "        scaler = MinMaxScaler()\n",
    "        dataset[normalize_columns] = scaler.fit_transform(dataset[normalize_columns])\n",
    "        print(f\"Columns normalized: {normalize_columns}\")\n",
    "        print(dataset[normalize_columns].head(5))\n",
    "    \n",
    "    # Remove original categorical column if specified\n",
    "    if remove_original:\n",
    "        dataset = dataset.drop(columns=[feature_name])\n",
    "        print(f\"Original feature '{feature_name}' removed.\")\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'Gender' encoded. Sample:\n",
      "                Gender  Gender_encoded\n",
      "Word                                  \n",
      "100-mètres   masculine               1\n",
      "2D            feminine               0\n",
      "3D            feminine               0\n",
      "µA           masculine               1\n",
      "a            masculine               1\n",
      "a b c        masculine               1\n",
      " a demi-mot  masculine               1\n",
      "a-mi-la      masculine               1\n",
      "aa           masculine               1\n",
      "aabam        masculine               1\n",
      "Original feature 'Gender' removed.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset and reassign the updated DataFrame\n",
    "nouns_cleaned = preprocess_dataset(\n",
    "    dataset=nouns_cleaned,\n",
    "    feature_name='Gender',\n",
    "    encode_as1='masculine',\n",
    "    normalize_columns=None,\n",
    "    remove_original=True  # Remove the categorical column\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Number       Lemma  Gender_encoded\n",
      "Word                                              \n",
      "100-mètres  invariable  100-mètres               1\n",
      "2D          invariable          2D               0\n",
      "3D          invariable          3D               0\n",
      "µA          invariable           A               1\n",
      "a           invariable           a               1\n"
     ]
    }
   ],
   "source": [
    "# Print the updated DataFrame\n",
    "print(nouns_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Number       Lemma  Gender_encoded\n",
      "Word                                              \n",
      "100-mètres  invariable  100-mètres               1\n",
      "2D          invariable          2D               0\n",
      "3D          invariable          3D               0\n",
      "µA          invariable           A               1\n",
      "a           invariable           a               1\n",
      "...                ...         ...             ...\n",
      "φ           invariable           φ               1\n",
      "χ           invariable           χ               1\n",
      "ψ           invariable           ψ               1\n",
      "ω           invariable           ω               1\n",
      "Ω           invariable           Ω               1\n",
      "\n",
      "[175708 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(nouns_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Word', 'Number', 'Lemma', 'Gender_encoded'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "nouns_cleaned = nouns_cleaned.reset_index()\n",
    "print(nouns_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_cleaned.to_csv(\"cleaned_nouns_gender.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "class WENotFound(Exception):\n",
    "    \"\"\"Exception raised when a word embedding is not found.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Function to load model and tokenizer\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Load a pre-trained masked language model and its tokenizer.\n",
    "    :param model_name: Name of the pre-trained model.\n",
    "    :return: Model and tokenizer objects.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to get word embedding\n",
    "def get_we(model, tokenizer, word):\n",
    "    \"\"\"\n",
    "    Extract the embedding for a single word.\n",
    "    :param model: Pre-trained model.\n",
    "    :param tokenizer: Tokenizer corresponding to the model.\n",
    "    :param word: Word to extract embedding for.\n",
    "    :return: Numpy array containing the word embedding.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer.encode(word)\n",
    "    if len(encoding) != 3:  # Word should be encoded as a single token\n",
    "        raise WENotFound(f'{word}: the word doesn\\'t exist in the vocab')\n",
    "\n",
    "    word_id = encoding[1]  # Extract the actual word token ID\n",
    "    token_ids = torch.tensor([[word_id]])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_ids, output_hidden_states=True)\n",
    "        last_layer_hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "    return last_layer_hidden_states.squeeze().numpy()\n",
    "\n",
    "# Function to generate embeddings for all words in dataset\n",
    "def generate_embeddings(words_df, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of words.\n",
    "    :param words_df: DataFrame containing words and their labels.\n",
    "    :param model: Pre-trained language model.\n",
    "    :param tokenizer: Corresponding tokenizer.\n",
    "    :return: DataFrame with word embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    skipped_words = []\n",
    "\n",
    "    for _, row in words_df.iterrows():\n",
    "        word = row[\"Word\"]\n",
    "        gender = row[\"Gender_encoded\"]\n",
    "\n",
    "        try:\n",
    "            word_embedding = get_we(model, tokenizer, word)\n",
    "            word_dict = {x[0]: x[1] for x in enumerate(word_embedding)}\n",
    "            word_dict['Word'] = word\n",
    "            word_dict['Gender'] = gender\n",
    "            embeddings.append(word_dict)\n",
    "\n",
    "        except WENotFound:\n",
    "            skipped_words.append(word)\n",
    "\n",
    "    print(f\"Skipped words due to out-of-vocabulary: {len(skipped_words)}\")\n",
    "    emb_df = pd.DataFrame(embeddings)\n",
    "    return emb_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing embeddings for XLM-R_large...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba2e337585e41e58ab602c05b93c57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5be8372d28348e6b13f85e31eaf81c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped words due to out-of-vocabulary: 171851\n",
      "Sample embeddings for XLM-R_large:\n",
      "                  0         1         2         3         4         5  \\\n",
      "Word                                                                    \n",
      "a          0.084792  0.019702  0.113618  0.132146 -0.027365 -0.116332   \n",
      "aba        0.314796 -0.127990  0.359627 -0.249409  0.044068 -0.172515   \n",
      "abandon    0.063121 -0.420607  0.473553 -0.230607 -0.404868  0.292548   \n",
      "abba      -0.225727  0.108689  0.235761 -0.107173  0.285700  0.018964   \n",
      "abdominal -0.035306 -0.068203  0.302270 -0.129037  0.472714 -0.213675   \n",
      "\n",
      "                  6         7         8         9  ...      1015      1016  \\\n",
      "Word                                               ...                       \n",
      "a          0.172065  0.042265  0.109370 -0.005246  ... -0.075809  0.073173   \n",
      "aba       -0.421966 -0.244339 -0.157397 -0.053287  ...  0.024180 -0.251885   \n",
      "abandon   -0.316067 -0.559579 -0.008430 -0.220676  ...  0.368944  0.107064   \n",
      "abba      -0.419519 -0.437339 -0.012233 -0.113955  ...  0.277057 -0.262965   \n",
      "abdominal -0.502867  0.159517 -0.176921 -0.145372  ...  0.047238  0.332361   \n",
      "\n",
      "               1017      1018      1019      1020      1021      1022  \\\n",
      "Word                                                                    \n",
      "a          0.022507  0.024762  0.067266 -0.044318  0.140157 -0.074929   \n",
      "aba       -0.006855 -0.242500  0.024299 -0.045655  0.009486  0.044734   \n",
      "abandon   -0.320699  0.371978 -0.380024 -0.586718  0.590762  0.352261   \n",
      "abba      -0.288050 -0.536879 -0.119782  0.019584 -0.109067  0.015708   \n",
      "abdominal -0.166508 -0.159517 -0.020466 -0.073194 -0.063935  0.098172   \n",
      "\n",
      "               1023  Gender  \n",
      "Word                         \n",
      "a          0.078578       1  \n",
      "aba        0.080146       1  \n",
      "abandon   -0.373271       1  \n",
      "abba      -0.321009       1  \n",
      "abdominal -0.089008       1  \n",
      "\n",
      "[5 rows x 1025 columns]\n",
      "Saved embeddings for XLM-R_large.\n",
      "Saved embeddings for XLM-R_large.\n",
      "Processing embeddings for XLM-Roberta-Base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d08360a34ce4360956093de3828d738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  62%|######2   | 692M/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user1\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user1\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped words due to out-of-vocabulary: 171851\n",
      "Sample embeddings for XLM-Roberta-Base:\n",
      "                  0         1         2         3         4         5  \\\n",
      "Word                                                                    \n",
      "a          0.076339  0.100680  0.046483 -0.019450  0.041517 -0.025529   \n",
      "aba        0.076340  0.100680  0.046483 -0.019450  0.041517 -0.025529   \n",
      "abandon    0.076338  0.100679  0.046478 -0.019451  0.041519 -0.025531   \n",
      "abba       0.076340  0.100676  0.046482 -0.019451  0.041515 -0.025529   \n",
      "abdominal  0.076337  0.100679  0.046470 -0.019452  0.041524 -0.025531   \n",
      "\n",
      "                  6         7         8         9  ...       759       760  \\\n",
      "Word                                               ...                       \n",
      "a          0.011361  0.001988  0.076578 -0.106401  ...  0.007592  0.071321   \n",
      "aba        0.011362  0.001988  0.076578 -0.106402  ...  0.007593  0.071321   \n",
      "abandon    0.011356  0.001991  0.076574 -0.106401  ...  0.007594  0.071314   \n",
      "abba       0.011363  0.001988  0.076579 -0.106400  ...  0.007593  0.071316   \n",
      "abdominal  0.011347  0.001995  0.076569 -0.106402  ...  0.007595  0.071308   \n",
      "\n",
      "                761       762       763       764       765       766  \\\n",
      "Word                                                                    \n",
      "a          0.004357  0.067423 -0.031071  0.127920 -0.120300  0.033458   \n",
      "aba        0.004358  0.067424 -0.031071  0.127920 -0.120299  0.033458   \n",
      "abandon    0.004357  0.067420 -0.031075  0.127921 -0.120307  0.033450   \n",
      "abba       0.004361  0.067422 -0.031071  0.127920 -0.120299  0.033455   \n",
      "abdominal  0.004354  0.067415 -0.031082  0.127922 -0.120320  0.033441   \n",
      "\n",
      "                767  Gender  \n",
      "Word                         \n",
      "a          0.028311       1  \n",
      "aba        0.028312       1  \n",
      "abandon    0.028316       1  \n",
      "abba       0.028315       1  \n",
      "abdominal  0.028320       1  \n",
      "\n",
      "[5 rows x 769 columns]\n",
      "Saved embeddings for XLM-Roberta-Base.\n",
      "Saved embeddings for XLM-Roberta-Base.\n",
      "Processing embeddings for mBERT-Base-Cased...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd45f12630a647e1a4c681ddaf7a0d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user1\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user1\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bb54d4beb5440e86af45051d92d3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e878ed6836748ba879ef3ff5ea094e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f0de41837846ceb4275a3a32ae5aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9ffc84c0ab4cf4a3f8e95112656078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped words due to out-of-vocabulary: 171379\n",
      "Sample embeddings for mBERT-Base-Cased:\n",
      "                0         1         2         3         4         5         6  \\\n",
      "Word                                                                            \n",
      "2D       0.370462 -0.543173  0.313777  0.114062  0.105755  0.101264  0.047496   \n",
      "3D       0.354639 -0.545033  0.300137  0.125183  0.109216  0.103105  0.037388   \n",
      "a        0.349609 -0.534463  0.286310  0.138533  0.116628  0.095070  0.029105   \n",
      "aa       0.350173 -0.535272  0.285707  0.138325  0.116960  0.096526  0.028733   \n",
      "abandon  0.348776 -0.534598  0.285159  0.137568  0.115842  0.096030  0.028421   \n",
      "\n",
      "                7         8         9  ...       759       760       761  \\\n",
      "Word                                   ...                                 \n",
      "2D       0.941036 -0.635095  0.351250  ...  1.044858 -0.638283 -0.908592   \n",
      "3D       0.945385 -0.627031  0.362872  ...  1.067076 -0.626916 -0.920606   \n",
      "a        0.942508 -0.610512  0.366846  ...  1.084296 -0.629184 -0.919081   \n",
      "aa       0.941748 -0.610730  0.365451  ...  1.083691 -0.628121 -0.917328   \n",
      "abandon  0.942981 -0.611037  0.366652  ...  1.083290 -0.628404 -0.915106   \n",
      "\n",
      "              762       763       764       765       766       767  Gender  \n",
      "Word                                                                         \n",
      "2D      -0.215859  0.016757  0.262812 -0.034462  0.003319 -0.339071       0  \n",
      "3D      -0.219173  0.017489  0.266173 -0.040272 -0.011564 -0.345399       0  \n",
      "a       -0.229349  0.016216  0.266248 -0.034348 -0.016950 -0.364401       1  \n",
      "aa      -0.228044  0.015919  0.266548 -0.035120 -0.017618 -0.364172       1  \n",
      "abandon -0.227728  0.015840  0.266958 -0.034558 -0.017089 -0.363497       1  \n",
      "\n",
      "[5 rows x 769 columns]\n",
      "Saved embeddings for mBERT-Base-Cased.\n",
      "Saved embeddings for mBERT-Base-Cased.\n",
      "Processing embeddings for DistilBERT-Base-Cased...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba467ef1f1b04beea1c27f7cd617df55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user1\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user1\\.cache\\huggingface\\hub\\models--distilbert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a5a32729d34b2b92f571a3daeb9522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202c27e32a0f4e8f8f05bbf58397c9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98dff32aa72d4b7d859688cedf1e09d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db31df84a32486baf4752435e522eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped words due to out-of-vocabulary: 171379\n",
      "Sample embeddings for DistilBERT-Base-Cased:\n",
      "                0         1         2         3         4         5         6  \\\n",
      "Word                                                                            \n",
      "2D       1.044108  0.413215  0.149576  0.126064 -0.028639  0.923040 -0.880763   \n",
      "3D       0.213090  0.312851  0.171519 -0.180463  0.048762  0.827893 -0.143109   \n",
      "a        0.079794 -0.093255  0.553697  0.045576  0.298140  0.063133 -0.170536   \n",
      "aa       0.152444 -0.343460  0.597466 -0.076894  0.045116  0.231373 -0.098664   \n",
      "abandon -0.031310 -0.131179  0.523863  0.114205  0.392207  0.006046 -0.230375   \n",
      "\n",
      "                7         8         9  ...       759       760       761  \\\n",
      "Word                                   ...                                 \n",
      "2D       0.325376 -0.161253 -0.032791  ...  0.767849 -0.777275 -0.218974   \n",
      "3D      -0.228023 -0.030123 -0.339493  ...  0.199764 -0.720399 -0.217338   \n",
      "a       -0.048963  0.281889 -0.084127  ...  0.019550 -0.349297 -0.277583   \n",
      "aa      -0.157934  0.176656 -0.092234  ... -0.070274 -0.219944 -0.189272   \n",
      "abandon  0.142431  0.245815 -0.057462  ...  0.005163 -0.444612 -0.163756   \n",
      "\n",
      "              762       763       764       765       766       767  Gender  \n",
      "Word                                                                         \n",
      "2D       0.060962 -0.967557  0.458480  0.598431  0.384215  0.211237       0  \n",
      "3D       0.434594  0.049750  0.312915  0.202386  0.309982  0.239986       0  \n",
      "a        0.034903  0.081864  0.204603  0.291637  0.111204  0.062702       1  \n",
      "aa       0.304796  0.064932  0.342632  0.479274  0.137084  0.109535       1  \n",
      "abandon  0.052099  0.134627  0.149787  0.400910  0.182496  0.028728       1  \n",
      "\n",
      "[5 rows x 769 columns]\n",
      "Saved embeddings for DistilBERT-Base-Cased.\n",
      "Saved embeddings for DistilBERT-Base-Cased.\n"
     ]
    }
   ],
   "source": [
    "# Define model names\n",
    "model_names = {\n",
    " #   \"TinyBERT\": \"huawei-noah/TinyBERT_General_4L_312D\",\n",
    "  #  \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "  #    \"FlauBERT-small\": \"flaubert/flaubert_small_cased\",\n",
    "    #\"CamemBERT-base\": \"camembert-base\"\n",
    "   \n",
    "   \"XLM-R_large\": \"xlm-roberta-large\",\n",
    "   \"XLM-Roberta-Base\": \"xlm-roberta-base\",\n",
    "   #\"mBERT-Base-Uncased\": \"bert-base-multilingual-uncased\",\n",
    "   \"mBERT-Base-Cased\": \"bert-base-multilingual-cased\",\n",
    "    \"DistilBERT-Base-Cased\": \"distilbert-base-multilingual-cased\"\n",
    "\n",
    "}\n",
    "# Load dataset (cleaned nouns with Gender labels)\n",
    "nouns_cleaned = pd.read_csv(\"cleaned_nouns_gender.csv\")\n",
    "\n",
    "# Generate embeddings for each model\n",
    "for model_label, model_name in model_names.items():\n",
    "    print(f\"Processing embeddings for {model_label}...\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    embeddings_df = generate_embeddings(nouns_cleaned, model, tokenizer)\n",
    "    \n",
    "    # Set Word as Index\n",
    "    embeddings_df.set_index(\"Word\", inplace=True)\n",
    "\n",
    "    # Display sample\n",
    "    print(f\"Sample embeddings for {model_label}:\")\n",
    "    print(embeddings_df.head())\n",
    "\n",
    "    # Save embeddings to CSV\n",
    "    embeddings_df.to_csv(f\"{model_label}_embeddings_with_gender.csv\")\n",
    "    print(f\"Saved embeddings for {model_label}.\")\n",
    "\n",
    "    # Save embeddings in pickle format\n",
    "    embeddings_df.to_pickle(f\"{model_label}_embeddings.pkl\")\n",
    "    print(f\"Saved embeddings for {model_label}.\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word         0         1         2         3         4         5         6  \\\n",
      "0   2D -0.458535  1.026120 -0.156639  0.282224 -1.437061  1.080444 -0.037819   \n",
      "1   3D  0.056050  0.356133  0.071018  0.155525 -0.240813  0.212123 -0.128014   \n",
      "2    a -0.528666  1.097722 -0.130273  0.290261 -1.492992  1.224025 -0.052981   \n",
      "3   aa -0.533585  1.105228 -0.131850  0.289372 -1.501637  1.285083 -0.051327   \n",
      "4  aba -0.512694  1.115770 -0.135617  0.293074 -1.500828  1.257268 -0.053031   \n",
      "\n",
      "          7         8  ...       759       760       761       762       763  \\\n",
      "0 -0.188312  0.308035  ... -0.639540 -0.797463  0.159385 -0.637154  0.302692   \n",
      "1 -0.157016  0.187442  ... -0.245784 -0.459303 -0.110808  0.154719 -0.240903   \n",
      "2 -0.201115  0.315308  ... -0.775459 -0.770062  0.065690 -0.722923  0.384350   \n",
      "3 -0.194557  0.318292  ... -0.794284 -0.759392  0.063101 -0.726022  0.362084   \n",
      "4 -0.197878  0.316445  ... -0.792604 -0.775942  0.053793 -0.740963  0.364370   \n",
      "\n",
      "        764       765       766       767  Gender  \n",
      "0  0.030184  0.458736  0.000162 -0.530310       0  \n",
      "1 -0.024753 -0.181563  0.074288 -0.315978       0  \n",
      "2  0.042721  0.442656  0.057813 -0.576737       1  \n",
      "3  0.047253  0.430878  0.047147 -0.602918       1  \n",
      "4  0.040957  0.445637  0.049642 -0.590916       1  \n",
      "\n",
      "[5 rows x 770 columns]\n",
      "Index(['Word', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
      "       ...\n",
      "       '759', '760', '761', '762', '763', '764', '765', '766', '767',\n",
      "       'Gender'],\n",
      "      dtype='object', length=770)\n"
     ]
    }
   ],
   "source": [
    " # File path to the nouns dataset\n",
    "file_path = r\"C:\\Users\\user1\\Desktop\\HarvestWE-main\\mBERT-Base-Uncased_embeddings_with_gender.csv\"\n",
    "\n",
    "dataset = pd.read_csv(file_path)\n",
    "print(dataset.head())\n",
    "print(dataset.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed dataset saved successfully at: C:\\Users\\user1\\Desktop\\HarvestWE-main\\mBERT-Base-Uncased_embeddings_fixed.csv\n",
      "             0         1         2         3         4         5         6  \\\n",
      "Word                                                                         \n",
      "2D   -0.458535  1.026120 -0.156639  0.282224 -1.437061  1.080444 -0.037819   \n",
      "3D    0.056050  0.356133  0.071018  0.155525 -0.240813  0.212123 -0.128014   \n",
      "a    -0.528666  1.097722 -0.130273  0.290261 -1.492992  1.224025 -0.052981   \n",
      "aa   -0.533585  1.105228 -0.131850  0.289372 -1.501637  1.285083 -0.051327   \n",
      "aba  -0.512694  1.115770 -0.135617  0.293074 -1.500828  1.257268 -0.053031   \n",
      "\n",
      "             7         8         9  ...       759       760       761  \\\n",
      "Word                                ...                                 \n",
      "2D   -0.188312  0.308035  0.545822  ... -0.639540 -0.797463  0.159385   \n",
      "3D   -0.157016  0.187442 -0.101623  ... -0.245784 -0.459303 -0.110808   \n",
      "a    -0.201115  0.315308  0.630490  ... -0.775459 -0.770062  0.065690   \n",
      "aa   -0.194557  0.318292  0.629212  ... -0.794284 -0.759392  0.063101   \n",
      "aba  -0.197878  0.316445  0.634682  ... -0.792604 -0.775942  0.053793   \n",
      "\n",
      "           762       763       764       765       766       767  Gender  \n",
      "Word                                                                      \n",
      "2D   -0.637154  0.302692  0.030184  0.458736  0.000162 -0.530310       0  \n",
      "3D    0.154719 -0.240903 -0.024753 -0.181563  0.074288 -0.315978       0  \n",
      "a    -0.722923  0.384350  0.042721  0.442656  0.057813 -0.576737       1  \n",
      "aa   -0.726022  0.362084  0.047253  0.430878  0.047147 -0.602918       1  \n",
      "aba  -0.740963  0.364370  0.040957  0.445637  0.049642 -0.590916       1  \n",
      "\n",
      "[5 rows x 769 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 📥 Load the saved embeddings CSV\n",
    "file_path = r\"C:\\Users\\user1\\Desktop\\HarvestWE-main\\mBERT-Base-Uncased_embeddings_with_gender.csv\"\n",
    "\n",
    "# ✅ Read CSV and set 'Word' as the index\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# ✅ Move 'Word' column to index (if it's not already set)\n",
    "if 'Word' in dataset.columns:\n",
    "    dataset.set_index('Word', inplace=True)\n",
    "\n",
    "# ✅ Save the fixed CSV\n",
    "fixed_file_path = r\"C:\\Users\\user1\\Desktop\\HarvestWE-main\\mBERT-Base-Uncased_embeddings_fixed.csv\"\n",
    "dataset.to_csv(fixed_file_path)\n",
    "\n",
    "print(f\"✅ Fixed dataset saved successfully at: {fixed_file_path}\")\n",
    "print(dataset.head())  # Preview to confirm the fix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
